{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Treinando MLP (scikit-learn) com GridSearch + holdout e avaliação completa\n",
    "# - Explora arquiteturas, ativações, otimizadores, batch_size, L2 (alpha) e early stopping\n",
    "# - Mantém pipeline de pré-processamento (imputação, OHE e padronização)\n",
    "# - Salva resultados e modelo\n",
    "# - (Opcional) NN do zero em NumPy para demonstrar o loop de treinamento\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import pathlib\n",
    "\n",
    "# =========================\n",
    "# Configurações gerais\n",
    "# =========================\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.15    # 70/15/15 ~> grid usa CV em cima dos 85% de treino/val\n",
    "CV_SPLITS = 5\n",
    "PRIMARY_SCORING_BINARY = \"roc_auc\"\n",
    "PRIMARY_SCORING_MULTICLASS = \"roc_auc_ovr\"\n",
    "path_data = pathlib.Path().cwd().parent / \"data\" \n",
    "\n",
    "DATA_PATH = path_data / 'train.csv'\n",
    "TARGET: Optional[str] = \"Churn\"       # defina aqui se já sabe a coluna alvo (ex.: \"Churn\")\n",
    "\n",
    "# Grade menor (rápida) vs maior (exaustiva)\n",
    "EXHAUSTIVE = True  # coloque True para explorar mais combinações\n",
    "\n",
    "# Onde salvar artefatos\n",
    "OUTPUT_DIR = \"./mlp_gridsearch_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utilidades\n",
    "# =========================\n",
    "COMMON_TARGET_CANDIDATES = [\n",
    "    \"churn\", \"Churn\", \"CHURN\",\n",
    "    \"target\", \"TARGET\", \"label\", \"Label\", \"y\",\n",
    "    \"Exited\", \"is_churn\", \"default\"\n",
    "]\n",
    "\n",
    "def infer_target_column(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Tenta inferir a coluna-alvo:\n",
    "    1) nomes comuns (acima);\n",
    "    2) alguma coluna binária com nome sugestivo;\n",
    "    3) por último, alguma coluna claramente binária.\n",
    "    \"\"\"\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in COMMON_TARGET_CANDIDATES:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "\n",
    "    # procura por colunas com 2 valores únicos (binárias)\n",
    "    binary_cols = []\n",
    "    for c in df.columns:\n",
    "        uniques = pd.Series(df[c].dropna().unique())\n",
    "        if len(uniques) == 2:\n",
    "            binary_cols.append(c)\n",
    "\n",
    "    # heurística: se tiver 'churn' no nome\n",
    "    for c in df.columns:\n",
    "        if \"churn\" in c.lower():\n",
    "            return c\n",
    "\n",
    "    if len(binary_cols) == 1:\n",
    "        return binary_cols[0]\n",
    "\n",
    "    if len(binary_cols) > 1:\n",
    "        # arbitrariamente pegue a primeira, mas avise\n",
    "        print(f\"[AVISO] Múltiplas colunas binárias candidatas: {binary_cols}. \"\n",
    "              f\"Usando {binary_cols[0]}. Defina TARGET manualmente para garantir.\")\n",
    "        return binary_cols[0]\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Não foi possível inferir a coluna-alvo. \"\n",
    "        \"Defina TARGET manualmente (ex.: TARGET = 'Churn').\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_preprocessor(df: pd.DataFrame, target_col: str) -> Tuple[ColumnTransformer, List[str], List[str]]:\n",
    "    \"\"\"Cria um ColumnTransformer:\n",
    "       - num: imputação mediana + padronização\n",
    "       - cat: imputação modo + OneHotEncoder (dense)\n",
    "    \"\"\"\n",
    "    # Se o alvo estiver no df, remova pra detectar features\n",
    "    feature_df = df.drop(columns=[target_col]) if target_col in df.columns else df\n",
    "\n",
    "    numeric_features = feature_df.select_dtypes(include=[\"number\", \"float\", \"int\", \"bool\"]).columns.tolist()\n",
    "    categorical_features = feature_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    # OneHotEncoder compat (sklearn >=1.2 usa sparse_output)\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=True))\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", ohe)\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    return preprocessor, numeric_features, categorical_features\n",
    "\n",
    "\n",
    "def primary_scoring_for_target(y: pd.Series) -> str:\n",
    "    \"\"\"Define métrica principal de refit.\"\"\"\n",
    "    n_unique = y.nunique(dropna=True)\n",
    "    if n_unique <= 2:\n",
    "        return PRIMARY_SCORING_BINARY\n",
    "    return PRIMARY_SCORING_MULTICLASS\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Carregar dados\n",
    "# =========================\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Shape do dataset: {df.shape}\")\n",
    "\n",
    "# Definir/Inferir target\n",
    "if TARGET is None:\n",
    "    TARGET = infer_target_column(df)\n",
    "print(f\"Coluna alvo: {TARGET}\")\n",
    "\n",
    "# Se o alvo vier como string/Yes/No etc., tente mapear para {0,1} se binário\n",
    "y_raw = df[TARGET]\n",
    "if y_raw.dtype == object:\n",
    "    # Tenta mapear strings comuns para binário\n",
    "    map_yes = {\"yes\": 1, \"y\": 1, \"true\": 1, \"sim\": 1, \"churn\": 1, \"1\": 1}\n",
    "    map_no = {\"no\": 0, \"n\": 0, \"false\": 0, \"nao\": 0, \"não\": 0, \"retained\": 0, \"0\": 0}\n",
    "    y_lower = y_raw.astype(str).str.lower().str.strip()\n",
    "    mapped = y_lower.map(lambda v: 1 if v in map_yes else (0 if v in map_no else np.nan))\n",
    "    # Se mapeou quase tudo, usa o mapeamento\n",
    "    if mapped.notna().mean() > 0.9:\n",
    "        y = mapped.astype(int)\n",
    "    else:\n",
    "        # Deixa como estava (pode ser multiclass ou stratification por string)\n",
    "        y = y_raw.copy()\n",
    "else:\n",
    "    y = y_raw.copy()\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Split: holdout de teste\n",
    "# =========================\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Split -> train/val: {X_train_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# =========================\n",
    "# Preprocessador + Pipeline\n",
    "# =========================\n",
    "preprocessor, num_cols, cat_cols = build_preprocessor(df, TARGET)\n",
    "\n",
    "base_clf = MLPClassifier(\n",
    "    max_iter=200,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tol=1e-4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"clf\", base_clf)\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# Grade de hiperparâmetros\n",
    "# =========================\n",
    "# Grades \"compactas\" (rápidas) e \"exaustivas\" (mais lentas).\n",
    "if not EXHAUSTIVE:\n",
    "    hidden_sizes = [(32,), (64,), (128,), (64, 32), (128, 64)]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    alphas = [1e-5, 1e-4, 1e-3]\n",
    "    lr_init_sgd = [1e-2, 1e-3]\n",
    "    lr_init_adam = [1e-3, 5e-4]\n",
    "else:\n",
    "    hidden_sizes = [(32,), (64,), (128,), (256,), (64, 32), (128, 64), (128, 64, 32), (256, 128)]\n",
    "    batch_sizes = [32, 64, 128, 256]\n",
    "    alphas = [1e-6, 1e-5, 1e-4, 1e-3]\n",
    "    lr_init_sgd = [5e-2, 1e-2, 5e-3, 1e-3]\n",
    "    lr_init_adam = [3e-3, 1e-3, 5e-4, 1e-4]\n",
    "\n",
    "param_grid = [\n",
    "    # SGD (mini-batch controlado por batch_size)\n",
    "    {\n",
    "        \"clf__solver\": [\"sgd\"],\n",
    "        \"clf__hidden_layer_sizes\": hidden_sizes,\n",
    "        \"clf__activation\": [\"relu\", \"tanh\", \"logistic\"],  # logistic == sigmoid\n",
    "        \"clf__batch_size\": batch_sizes,\n",
    "        \"clf__learning_rate\": [\"constant\", \"adaptive\"],\n",
    "        \"clf__learning_rate_init\": lr_init_sgd,\n",
    "        \"clf__momentum\": [0.0, 0.9],\n",
    "        \"clf__nesterovs_momentum\": [True, False],\n",
    "        \"clf__alpha\": alphas,\n",
    "        \"clf__early_stopping\": [True, False],  # explorar uso de early stopping\n",
    "        \"clf__validation_fraction\": [0.15],\n",
    "    },\n",
    "    # Adam\n",
    "    {\n",
    "        \"clf__solver\": [\"adam\"],\n",
    "        \"clf__hidden_layer_sizes\": hidden_sizes,\n",
    "        \"clf__activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "        \"clf__batch_size\": batch_sizes,\n",
    "        \"clf__learning_rate_init\": lr_init_adam,\n",
    "        \"clf__alpha\": alphas,\n",
    "        \"clf__beta_1\": [0.9],\n",
    "        \"clf__beta_2\": [0.999],\n",
    "        \"clf__epsilon\": [1e-8],\n",
    "        \"clf__early_stopping\": [True, False],\n",
    "        \"clf__validation_fraction\": [0.15],\n",
    "    },\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Scoring / CV\n",
    "# =========================\n",
    "PRIMARY_SCORING = primary_scoring_for_target(y_train_val)\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc\" if y_train_val.nunique() <= 2 else \"roc_auc_ovr\",\n",
    "    \"average_precision\": \"average_precision\",\n",
    "    \"f1\": \"f1_macro\" if y_train_val.nunique() > 2 else \"f1\",\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=CV_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# =========================\n",
    "# Pesos amostrais (balancear classes) – opcional mas útil\n",
    "# =========================\n",
    "try:\n",
    "    sample_weights = compute_sample_weight(class_weight=\"balanced\", y=y_train_val)\n",
    "    fit_params = {\"clf__sample_weight\": sample_weights}\n",
    "except Exception:\n",
    "    sample_weights = None\n",
    "    fit_params = {}\n",
    "\n",
    "# =========================\n",
    "# Grid Search\n",
    "# =========================\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"roc_auc\" if y_train_val.nunique() <= 2 else \"roc_auc_ovr\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "\n",
    "print(f\"Iniciando GridSearchCV com métrica de refit: {grid.refit}\")\n",
    "grid.fit(X_train_val, y_train_val, **fit_params)\n",
    "\n",
    "# Resultados do grid\n",
    "cv_results_df = pd.DataFrame(grid.cv_results_).sort_values(by=f\"mean_test_{grid.refit}\", ascending=False)\n",
    "cv_results_path = os.path.join(OUTPUT_DIR, \"gridsearch_results.csv\")\n",
    "cv_results_df.to_csv(cv_results_path, index=False)\n",
    "print(f\"Resultados do GridSearch salvos em: {cv_results_path}\")\n",
    "\n",
    "print(\"\\n=== Melhor conjunto de hiperparâmetros (CV) ===\")\n",
    "print(json.dumps(grid.best_params_, indent=2))\n",
    "print(f\"\\nMelhor {grid.refit} (CV): {grid.best_score_:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Avaliação no Teste\n",
    "# =========================\n",
    "best_model: Pipeline = grid.best_estimator_\n",
    "\n",
    "# Transform para obter probabilidades\n",
    "y_pred = best_model.predict(X_test)\n",
    "proba_ok = False\n",
    "try:\n",
    "    y_proba = best_model.predict_proba(X_test)\n",
    "    proba_ok = True\n",
    "except Exception:\n",
    "    y_proba = None\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "if y_train_val.nunique() <= 2:\n",
    "    roc = roc_auc_score(y_test, y_proba[:, 1]) if proba_ok else np.nan\n",
    "    ap = average_precision_score(y_test, y_proba[:, 1]) if proba_ok else np.nan\n",
    "else:\n",
    "    roc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\") if proba_ok else np.nan\n",
    "    ap = average_precision_score(pd.get_dummies(y_test), y_proba) if proba_ok else np.nan\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average=\"binary\" if y_train_val.nunique() <= 2 else \"macro\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Métricas no conjunto de TESTE ===\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"ROC AUC  : {roc:.4f}\" if not np.isnan(roc) else \"ROC AUC  : n/a (sem predict_proba)\")\n",
    "print(f\"AvgPrec  : {ap:.4f}\" if not np.isnan(ap) else \"AvgPrec  : n/a (sem predict_proba)\")\n",
    "print(f\"F1       : {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "metrics_path = os.path.join(OUTPUT_DIR, \"test_metrics.json\")\n",
    "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"accuracy\": acc,\n",
    "        \"roc_auc\": None if np.isnan(roc) else roc,\n",
    "        \"average_precision\": None if np.isnan(ap) else ap,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"best_params\": grid.best_params_\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Métricas de teste salvas em: {metrics_path}\")\n",
    "\n",
    "# =========================\n",
    "# Curva de perda (treino) do melhor MLP\n",
    "# =========================\n",
    "try:\n",
    "    clf_best: MLPClassifier = best_model.named_steps[\"clf\"]\n",
    "    loss_curve = getattr(clf_best, \"loss_curve_\", None)\n",
    "    if loss_curve is not None and len(loss_curve) > 0:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(range(1, len(loss_curve) + 1), loss_curve, linewidth=2)\n",
    "        plt.xlabel(\"Época\")\n",
    "        plt.ylabel(\"Loss (treino)\")\n",
    "        plt.title(\"Curva de perda - melhor MLP\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        fig_path = os.path.join(OUTPUT_DIR, \"best_model_loss_curve.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(fig_path, dpi=150)\n",
    "        plt.close()\n",
    "        print(f\"Curva de perda salva em: {fig_path}\")\n",
    "except Exception as e:\n",
    "    print(\"[AVISO] Não foi possível plotar a loss_curve:\", e)\n",
    "\n",
    "# =========================\n",
    "# Salvar modelo final\n",
    "# =========================\n",
    "model_path = os.path.join(OUTPUT_DIR, \"best_mlp_pipeline.joblib\")\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Pipeline (pré-processamento + MLP) salvo em: {model_path}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# -----------------------------------------------\n",
    "# (Opcional) Rede Neural do zero (NumPy) – 1 camada oculta\n",
    "# Para demonstrar forward -> loss -> backprop -> atualização de pesos\n",
    "# -----------------------------------------------\n",
    "\n",
    "# %%\n",
    "RUN_SCRATCH = False  # ajuste para True se quiser treinar a NN do zero para comparação\n",
    "\n",
    "if RUN_SCRATCH:\n",
    "    # Reaproveita o pré-processador do pipeline para obter X numérico já imputado/escalado e OHE\n",
    "    X_proc = best_model.named_steps[\"pre\"].fit_transform(X_train_val)\n",
    "    y_proc = y_train_val.values\n",
    "\n",
    "    # Para binário, encoder para {0,1}\n",
    "    if y_train_val.nunique() == 2:\n",
    "        # Se não for int/float, converte via fatoração\n",
    "        if not np.issubdtype(y_proc.dtype, np.number):\n",
    "            _, y_proc = np.unique(y_proc, return_inverse=True)\n",
    "        y_proc = y_proc.astype(np.float64).reshape(-1, 1)\n",
    "        n_out = 1\n",
    "        is_binary = True\n",
    "    else:\n",
    "        # One-hot para multiclass\n",
    "        classes, y_idx = np.unique(y_proc, return_inverse=True)\n",
    "        n_out = len(classes)\n",
    "        y_onehot = np.eye(n_out)[y_idx]\n",
    "        y_proc = y_onehot.astype(np.float64)\n",
    "        is_binary = False\n",
    "\n",
    "    # Split interno para validação do early stopping\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_proc, y_proc, test_size=0.15, stratify=y_train_val, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Hiperparâmetros simples\n",
    "    n_in = X_tr.shape[1]\n",
    "    n_hidden = 64\n",
    "    epochs = 100\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    l2 = 1e-4\n",
    "    patience = 10\n",
    "    activation = \"relu\"  # \"relu\" | \"tanh\" | \"sigmoid\"\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "    # Inicialização Xavier/He simples\n",
    "    def init_weights(fan_in, fan_out, nonlin: str):\n",
    "        if nonlin == \"relu\":\n",
    "            std = math.sqrt(2.0 / fan_in)\n",
    "        else:\n",
    "            std = math.sqrt(1.0 / fan_in)\n",
    "        return rng.normal(0.0, std, size=(fan_in, fan_out))\n",
    "\n",
    "    W1 = init_weights(n_in, n_hidden, activation)\n",
    "    b1 = np.zeros((1, n_hidden))\n",
    "    W2 = init_weights(n_hidden, n_out, activation)\n",
    "    b2 = np.zeros((1, n_out))\n",
    "\n",
    "    def act(x, kind):\n",
    "        if kind == \"relu\":\n",
    "            return np.maximum(0, x)\n",
    "        if kind == \"tanh\":\n",
    "            return np.tanh(x)\n",
    "        if kind == \"sigmoid\":\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "        raise ValueError(kind)\n",
    "\n",
    "    def act_grad(x, kind):\n",
    "        if kind == \"relu\":\n",
    "            return (x > 0).astype(x.dtype)\n",
    "        if kind == \"tanh\":\n",
    "            y = np.tanh(x)\n",
    "            return 1 - y**2\n",
    "        if kind == \"sigmoid\":\n",
    "            s = 1.0 / (1.0 + np.exp(-x))\n",
    "            return s * (1 - s)\n",
    "        raise ValueError(kind)\n",
    "\n",
    "    def forward(Xb):\n",
    "        Z1 = Xb @ W1 + b1\n",
    "        A1 = act(Z1, activation)\n",
    "        Z2 = A1 @ W2 + b2\n",
    "        if is_binary:\n",
    "            # sigmoid + BCE\n",
    "            A2 = 1.0 / (1.0 + np.exp(-Z2))\n",
    "        else:\n",
    "            # softmax\n",
    "            Z2_shift = Z2 - Z2.max(axis=1, keepdims=True)\n",
    "            expZ = np.exp(Z2_shift)\n",
    "            A2 = expZ / expZ.sum(axis=1, keepdims=True)\n",
    "        cache = (Xb, Z1, A1, Z2, A2)\n",
    "        return A2, cache\n",
    "\n",
    "    def compute_loss(A2, yb):\n",
    "        # Cross-entropy + L2\n",
    "        eps = 1e-12\n",
    "        if is_binary:\n",
    "            # BCE\n",
    "            loss = -np.mean(yb * np.log(A2 + eps) + (1 - yb) * np.log(1 - A2 + eps))\n",
    "        else:\n",
    "            loss = -np.mean(np.sum(yb * np.log(A2 + eps), axis=1))\n",
    "        l2_term = 0.5 * l2 * (np.sum(W1**2) + np.sum(W2**2))\n",
    "        return loss + l2_term\n",
    "\n",
    "    def backward(cache, A2, yb):\n",
    "        Xb, Z1, A1, Z2, _ = cache\n",
    "        m = Xb.shape[0]\n",
    "\n",
    "        if is_binary:\n",
    "            dZ2 = (A2 - yb) / m  # derivada BCE + sigmoid\n",
    "        else:\n",
    "            dZ2 = (A2 - yb) / m  # derivada CE + softmax\n",
    "\n",
    "        dW2 = A1.T @ dZ2 + l2 * W2\n",
    "        db2 = dZ2.sum(axis=0, keepdims=True)\n",
    "\n",
    "        dA1 = dZ2 @ W2.T\n",
    "        dZ1 = dA1 * act_grad(Z1, activation)\n",
    "        dW1 = Xb.T @ dZ1 + l2 * W1\n",
    "        db1 = dZ1.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def iterate_minibatches(Xa, ya, bs, shuffle=True):\n",
    "        idx = np.arange(Xa.shape[0])\n",
    "        if shuffle:\n",
    "            rng.shuffle(idx)\n",
    "        for start in range(0, Xa.shape[0], bs):\n",
    "            sl = idx[start:start+bs]\n",
    "            yield Xa[sl], ya[sl]\n",
    "\n",
    "    best_val = np.inf\n",
    "    best_params = None\n",
    "    patience_cnt = 0\n",
    "    train_curve, val_curve = [], []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Treino\n",
    "        for Xb, yb in iterate_minibatches(X_tr, y_tr, batch_size, shuffle=True):\n",
    "            A2, cache = forward(Xb)\n",
    "            dW1, db1, dW2, db2 = backward(cache, A2, yb)\n",
    "            # Atualização (SGD)\n",
    "            W1 -= lr * dW1\n",
    "            b1 -= lr * db1\n",
    "            W2 -= lr * dW2\n",
    "            b2 -= lr * db2\n",
    "\n",
    "        # Avalia loss\n",
    "        A2_tr, _ = forward(X_tr)\n",
    "        A2_val, _ = forward(X_val)\n",
    "        loss_tr = compute_loss(A2_tr, y_tr)\n",
    "        loss_val = compute_loss(A2_val, y_val)\n",
    "        train_curve.append(loss_tr)\n",
    "        val_curve.append(loss_val)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f\"[NumPy NN] Época {epoch:03d} | loss_tr={loss_tr:.4f} | loss_val={loss_val:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if loss_val < best_val - 1e-5:\n",
    "            best_val = loss_val\n",
    "            best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy())\n",
    "            patience_cnt = 0\n",
    "        else:\n",
    "            patience_cnt += 1\n",
    "            if patience_cnt >= patience:\n",
    "                print(f\"[NumPy NN] Early stopping na época {epoch}\")\n",
    "                break\n",
    "\n",
    "    if best_params is not None:\n",
    "        W1, b1, W2, b2 = best_params\n",
    "\n",
    "    # Curvas\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_curve, label=\"treino\")\n",
    "    plt.plot(val_curve, label=\"val\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Curva de perda - NumPy NN\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    np_fig_path = os.path.join(OUTPUT_DIR, \"numpy_nn_loss_curve.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(np_fig_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[NumPy NN] Curva de perda salva em: {np_fig_path}\")\n",
    "\n",
    "    # Avaliação no teste (usando mesmo pré-processador)\n",
    "    X_test_proc = best_model.named_steps[\"pre\"].transform(X_test)\n",
    "    A2_test, _ = forward(X_test_proc)\n",
    "    if is_binary:\n",
    "        y_pred_np = (A2_test.ravel() >= 0.5).astype(int)\n",
    "        y_proba_np = A2_test.ravel()\n",
    "        acc_np = accuracy_score(y_test, y_pred_np)\n",
    "        roc_np = roc_auc_score(y_test, y_proba_np)\n",
    "        ap_np = average_precision_score(y_test, y_proba_np)\n",
    "        f1_np = f1_score(y_test, y_pred_np)\n",
    "        \n",
    "        print(f\"[NumPy NN] Test -> acc={acc_np:.4f} | roc_auc={roc_np:.4f} | ap={ap_np:.4f} | f1={f1_np:.4f}\")\n",
    "    else:\n",
    "        y_pred_np = A2_test.argmax(axis=1)\n",
    "        # mapear rótulos de volta se necessário – omitido por simplicidade\n",
    "        acc_np = accuracy_score(y_test, y_pred_np)\n",
    "        f1_np = f1_score(y_test, y_pred_np, average=\"macro\")\n",
    "        print(f\"[NumPy NN] Test -> acc={acc_np:.4f} | f1_macro={f1_np:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curstomer-churn (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
